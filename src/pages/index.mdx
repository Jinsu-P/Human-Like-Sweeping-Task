---
layout: ../layouts/Layout.astro
title: Human-Like Sweeping with Collaborative Robots via Trajectory Modeling from User Demonstrations and Language Models
description: Simple project page template for your research paper, built with Astro and Tailwind CSS
favicon: iCAT_logo.svg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";

import demo from "../assets/demo_sweep.mp4";
import demonstration_vel from "../assets/demonstration_vel.png";
import human_sweep_ori from "../assets/human_sweeping_ori.png";
import human_sweep_change from "../assets/human_sweeping_change.png";
import overall_exp from "../assets/overall.png";
import teleoperation_vive from "../assets/vive_teleoperation.png";
import exp_first from "../assets/first_step.png";
import exp_second from "../assets/second_step.png";
import original_indy7 from "../assets/original_indy7_ver2.mp4";
import original_3D from "../assets/original_tpos_2.mp4";
import sweep_num from "../assets/1st_indy7.mp4";
import sweep_num_3D from "../assets/1st_tpos.mp4";
import sweep_move from "../assets/2nd_indy7.mp4";
import sweep_move_3D from "../assets/2nd_tpos.mp4";
import sweep_start_point from "../assets/3rd_indy7.mp4";
import sweep_start_point_3D from "../assets/3rd_tpos.mp4";
import sweep_speed from "../assets/4th_indy7.mp4";
import sweep_speed_3D from "../assets/4th_tpos.mp4";
import scenario1 from "../assets/animation_execution_plot.mp4";
import transformer from "../assets/transformer.webp";
import Splat from "../components/Splat.tsx"
import dogsDiffc from "../assets/dogs-diffc.png"
import dogsTrue from "../assets/dogs-true.png"


import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Jin-su Park",
      //url: "https://roman.technology",
      notes: ["1*"],
    },
    {
      name: "Sanghon Song",
      //institution: "Institution Two",
      notes: ["1"],
    },
    {
      name: "Jungwoo Kim",
      //institution: "Institution Three",
      notes: ["1"],
    },
    {
      name: "Dongyeop Kang",
      notes: ["2"],
    },
    {
      name: "Jeyoun Dong",
      notes: ["2"],
    },
    {
      name: "Chan-eun Park",
      notes: ["1â€ "]
    }
  ]}
  notes={[
    {
      symbol: "1",
      text: "Kyungpook National University",
    },
    {
      symbol: "2",
      text: "Electronic and Telecommunications Research Institute",
    },
  ]}
  links={[
    {
      name: "Paper",
      url: "",
      icon: "ri:file-pdf-2-line",
    },
    {
      name: "Code",
      url: "",
      icon: "ri:github-line",
    },
    {
      name: "arXiv",
      url: "",
      icon: "academicons:arxiv",
    },
  ]}
  />

<Video source={demo} />

<HighlightedSection>

## Abstract

This study proposes a trajectory modeling framework using user-demonstrated motion for collaborative robots to perform human-like sweeping tasks. By collecting the robot's motion data through a remote controller, we modeled the sweeping motion with a polynomial dynamical system (PLYDS). Additionally, we used large language models (LLMs) to specify sweeping tasks via natural language. Our framework allows users to adjust the robot's behavior without requiring any programming or domain expertise. Experimental results using an industrial collaborative robot demonstrated that the proposed approach not only produced human-like sweeping motions, but also enabled non-expert users to operate the system intuitively.

</HighlightedSection>

## Motivation

Sweeping tasks in outdoor environments such as road cleaning and industrial complexes are essential for ensuring worker safety and maintaining public hygiene. However, due to challenging weather conditions, the risk of traffic accidents, insufficient protective equipment, and a shortage of labor, such tasks are often hazardous and difficult for humans to perform directly.
As a result, there is a growing need to automate these operations using collaborative robots.
In particular, a teleoperation-based, programming-less framework is required to enable non-expert users to intuitively control the robot.
In our previous study, we proposed a **waypoint-based trajectory generation algorithm using a handheld controller and a mobile application**.
Usability evaluations demonstrated the effectiveness of this method in improving the accessibility of collaborative robot control. However, the approach was limited in its ability to express diverse motions and lacked flexibility in adapting to dynamic envirionment changes.
To address these limitations, the present study proposes a framework that enables collaborative robots to perform sweeping tasks more naturally and realistically by **modeling human motion using a Polynomial Dynamical System (PLYDS)** and supporting intuitive **trajectory modification through a Large Language Model (LLM)-based** natrual language interface.

## Human Sweeping Motion

Velocity data were obtained from actual human sweeping demonstrations performed via a collaborative robot and a remote controller.
The measured profiles exhibited a shape closely matching a sine wave, which motivated teh selection fo a sine-based velocity profile in this study.

<Figure>
  <Image slot="figure" source={demonstration_vel} altText="Human sweeping demonstration velocity profile" />
  <span slot="caption">Human sweeping demonstration velocity profile</span>
</Figure>

<TwoColumns>
  <Figure slot="left">
    <Image slot="figure" source={human_sweep_ori} altText="Original robot's sweeping motion velocity" />
    <span slot="caption">Original robot's sweeping motion velocity</span>
  </Figure>
  <Figure slot="right">
    <Image slot="figure" source={human_sweep_change} altText="Human-Like sweeping motion velocity" />
    <span slot="caption">Human-Like sweeping motion velocity</span>
  </Figure>
</TwoColumns>

## Experiments

<TwoColumns>
  <Figure slot="left">
    <Image slot="figure" source={overall_exp} altText="Overview" />
    <span slot="caption">Overview</span>
  </Figure>
  <Figure slot="right">
    <Image slot="figure" source={teleoperation_vive} altText="Collecting sweeping trajectory data" />
    <span slot="caption">Collecting sweeping trajectory data</span>
  </Figure>
</TwoColumns>

We used a 6-DoF robot (Neuromeka Indy7) and a remote controller (HTC Vive Pro Controller) to collect 100 sweeping demonstrations, which were then used to train the PLYDS model.

**Basic Sweeping Task of Indy7**

<TwoColumns>
  <Figure slot="left">
    <Video slot="figure" source={original_indy7} />
    <span slot="caption">Basic sweeping task</span>
  </Figure>
  <Figure slot="right">
    <Video slot="figure" source={original_3D} />
    <span slot="caption">3D Visualization</span>
  </Figure>
</TwoColumns>

**Adjust the Number of Passes in the Sweeping Trajectory**

<TwoColumns>
  <Figure slot="left">
    <Video slot="figure" source={sweep_num} />
    <span slot="caption">Number of passes : 3 -> 4 </span>
  </Figure>
  <Figure slot="right">
    <Video slot="figure" source={sweep_num_3D} />
    <span slot="caption">3D Visualization</span>
  </Figure>
</TwoColumns>

**Move the Entire Sweeping Trajectory**

<TwoColumns>
  <Figure slot="left">
    <Video slot="figure" source={sweep_move} />
    <span slot="caption">Sweep Trajectory move to -1.5cm(z axis) </span>
  </Figure>
  <Figure slot="right">
    <Video slot="figure" source={sweep_move_3D} />
    <span slot="caption">3D Visualization</span>
  </Figure>
</TwoColumns>

**Modify the Starting Point of the Sweeping Trajectory**

<TwoColumns>
  <Figure slot="left">
    <Video slot="figure" source={sweep_start_point} />
    <span slot="caption">Starting Point modify to 2cm(x axis) </span>
  </Figure>
  <Figure slot="right">
    <Video slot="figure" source={sweep_start_point_3D} />
    <span slot="caption">3D Visualization</span>
  </Figure>
</TwoColumns>

**Adjust the Sweeping Motion Speed**

<TwoColumns>
  <Figure slot="left">
    <Video slot="figure" source={sweep_speed} />
    <span slot="caption">Sweeping motion speed x1.5 </span>
  </Figure>
  <Figure slot="right">
    <Video slot="figure" source={sweep_speed_3D} />
    <span slot="caption">3D Visualization</span>
  </Figure>
</TwoColumns>

**Main Scenario**

<Figure>
  <Image slot="figure" source={exp_first} altText="Applying PLYDS model to Sweeping Task" />
  <span slot="caption">Applying PLYDS model to Sweeping Task</span>
</Figure>

After checking the floor condition, four waypoints wrer designated using the Vive Controller to define the desired workspace.
Waypoint-based trajectory generation algorithm was then used to interpolate the entire trajectory
PLYDS model was used to execute sweeping motions between the waypoints

<Figure>
  <Image slot="figure" source={exp_second} altText="Sweeping Trajectory Modification" />
  <span slot="caption">Sweeping Trajectory Modification</span>
</Figure>

When additional cleaning was required, user input (e.g., "Sweep deeper, one more time").
A new trajectory was generated and executed without any demonstrations


## Conclusion

In this study, we proposed a trajectory meodeling and modification framework for collaborative robots using PLYDS and LLMs.
It improves similarity to human motion while remaining easy to use for non-experts.
This approach enhances practical usability, and future work will explore deployment on mobile manipulators for outdoor sweeping tasks.

## Figures

Use the figure component to display images, videos, equations, or any other element, with an optional caption.

<Figure>
  <Image slot="figure" source={transformer} altText="Diagram of the transformer deep learning architecture." />
  <span slot="caption">Diagram of the transformer deep learning architecture.</span>
</Figure>

## Image comparison slider

An interactive, accessible slider component with keyboard navigation.
<Figure>
  <ImageComparison slot="figure" client:load imageUrlOne={dogsDiffc.src} imageUrlTwo={dogsTrue.src} altTextOne="Photo of two dogs running side-by-side in shallow water, lossily compressed using the DiffC algorithm" altTextTwo="Original photo of two dogs running side-by-side in shallow water" />
  <span slot="caption">A photo of two dogs running side-by-side in shallow water, lossily compressed using the <a href="https://jeremyiv.github.io/diffc-project-page/">DiffC algorithm</a>.</span>
</Figure>

## Two columns

Use the two columns component to display two columns of content. In this example, the first column contains a figure with a YouTube video and the second column contains a figure with a custom [React](https://react.dev/) component. By default, they display side by side, but if the screen is narrow enough (for example, on mobile), they're arranged vertically.

<TwoColumns>
  <Figure slot="left">
    <YouTubeVideo slot="figure" videoId="wjZofJX0v4M" />
    <span slot="caption">Take a look at this YouTube video.</span>
  </Figure>
  <Figure slot="right">
    <Splat slot="figure" client:idle />
    <span slot="caption">Now look at this <a href="https://en.wikipedia.org/wiki/Gaussian_splatting">Gaussian splat</a>, rendered with a React component.</span>
  </Figure>
</TwoColumns>

## Heading levels

Use headings to divide your content into sections.

### Heading 3

Go down a level to heading 3...

#### Heading 4

...and down again to heading 4.

## LaTeX

You can also add LaTeX formulas, rendered during the build process using [KaTeX](https://katex.org/) so they're quick to load for visitors of your project page. You can write them inline, like this: <LaTeX inline formula="a^2 + b^2 = c^2" />. Or, you can write them as a block:

<LaTeX formula="\int_a^b f(x) dx" />


## Tables

You can add simple tables using [GitHub Flavored Markdown syntax](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/organizing-information-with-tables):

| Model | Accuracy | F1 score | Training time (hours) |
| :--- | :---: | :---: | :---: |
| BERT-base | 0.89 | 0.87 | 4.5 |
| RoBERTa-large | 0.92 | 0.91 | 7.2 |
| DistilBERT | 0.86 | 0.84 | 2.1 |
| XLNet | 0.90 | 0.89 | 6.8 |



## BibTeX citation

```bibtex
@article{park2025,
  author = "{Park, Jin-su and Song, Sanghun and Kim, Jungwoo and Kang, Dongyeop and Dong, Jeyoun and Park, Cahn-eun}",
  title = "Human-Like Sweeping with Collaborative Robots via Trajectory Modeling from User Demonstrations and Language Models",
  year = "2025",
}
```